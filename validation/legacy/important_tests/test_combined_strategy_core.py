#!/usr/bin/env python3
"""
Phase 1 Critical Test: COMBINED Strategy Core (Test 10/12)

Tests the FLAGSHIP COMBINED continual learning strategy:
- Adaptive integration of EWC + Replay + Progressive + Adapters
- Dynamic strategy selection based on task characteristics
- Interference detection and automatic mitigation
- Superior performance vs individual strategies

Competitive Advantage - FLAGSHIP FEATURE:
This is SymbioAI's SECRET SAUCE. COMBINED strategy automatically
orchestrates multiple continual learning techniques, outperforming
any single method. SakanaAI doesn't have this.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import torch
import numpy as np
from comprehensive_benchmark import (
    SymbioAICombined,
    create_continual_learning_system,
    Task
)


class TestCOMBINEDStrategyCore:
    """Critical tests for COMBINED strategy - THE FLAGSHIP."""
    
    def test_combined_strategy_creation(self):
        """Test 10.1: Create COMBINED strategy system."""
        combined = create_continual_learning_system(
            strategy='combined',
            input_dim=128,
            hidden_dim=256,
            output_dim=10
        )
        
        assert combined is not None, "COMBINED strategy creation failed"
        assert isinstance(combined, SymbioAICombined), "Wrong type"
        
        # Should have ALL components
        assert hasattr(combined, 'ewc'), "Missing EWC component"
        assert hasattr(combined, 'replay_buffer'), "Missing replay component"
        assert hasattr(combined, 'progressive_columns'), "Missing progressive nets"
        assert hasattr(combined, 'adapters'), "Missing adapter component"
    
    def test_adaptive_strategy_selection(self):
        """Test 10.2: COMBINED automatically selects best sub-strategy."""
        combined = create_continual_learning_system(strategy='combined')
        
        # Create different task types
        easy_task = Task(
            id="easy_task",
            name="Simple Classification",
            description="Easy classification with clear boundaries",
            dataset_name="easy_data",
            num_classes=3,
            difficulty=0.2
        )
        
        hard_task = Task(
            id="hard_task",
            name="Complex Multi-Class",
            description="Difficult classification with overlapping classes",
            dataset_name="hard_data",
            num_classes=20,
            difficulty=0.9
        )
        
        # Register tasks
        combined.register_task(easy_task)
        combined.register_task(hard_task)
        
        # COMBINED should select appropriate strategy for each
        easy_strategy = combined.select_strategy(easy_task)
        hard_strategy = combined.select_strategy(hard_task)
        
        assert easy_strategy is not None, "No strategy for easy task"
        assert hard_strategy is not None, "No strategy for hard task"
        
        # Strategies may differ based on task characteristics
        print(f"   Easy task strategy: {easy_strategy}")
        print(f"   Hard task strategy: {hard_strategy}")
    
    def test_interference_detection_and_mitigation(self):
        """Test 10.3: Detect and mitigate catastrophic forgetting."""
        combined = create_continual_learning_system(strategy='combined')
        
        # Train on first task
        task1 = Task(id="t1", name="Task 1", num_classes=5, difficulty=0.5)
        combined.register_task(task1)
        
        # Simulate training
        for epoch in range(5):
            data = torch.randn(32, combined.model.input_dim)
            labels = torch.randint(0, 5, (32,))
            
            loss = combined.train_step(data, labels, task_id="t1")
        
        # Measure performance on task 1
        task1_performance_before = combined.evaluate(task1)
        
        # Train on second task (potential interference)
        task2 = Task(id="t2", name="Task 2", num_classes=5, difficulty=0.5)
        combined.register_task(task2)
        
        for epoch in range(5):
            data = torch.randn(32, combined.model.input_dim)
            labels = torch.randint(0, 5, (32,))
            
            loss = combined.train_step(data, labels, task_id="t2")
        
        # Check if interference was detected
        interference = combined.detect_interference(task1, task2)
        
        assert interference is not None, "Interference detection failed"
        assert 'detected' in interference or 'interference_level' in interference, \
            "No interference assessment"
        
        # Measure performance on task 1 again
        task1_performance_after = combined.evaluate(task1)
        
        # COMBINED should prevent catastrophic forgetting
        # Performance shouldn't drop significantly
        performance_drop = task1_performance_before - task1_performance_after
        print(f"   Performance drop: {performance_drop:.3f}")
    
    def test_multi_component_integration(self):
        """Test 10.4: All components work together seamlessly."""
        combined = create_continual_learning_system(strategy='combined')
        
        # Register multiple tasks
        tasks = [
            Task(id=f"t{i}", name=f"Task {i}", num_classes=5, difficulty=0.5)
            for i in range(3)
        ]
        
        for task in tasks:
            combined.register_task(task)
        
        # Train across all tasks
        for task in tasks:
            for epoch in range(3):
                data = torch.randn(16, combined.model.input_dim)
                labels = torch.randint(0, 5, (16,))
                
                loss = combined.train_step(data, labels, task_id=task.id)
                
                assert loss is not None, f"Training failed for {task.id}"
        
        # Verify all components were used
        # EWC: Fisher info should be computed
        assert len(combined.ewc.fisher_information) > 0, "EWC not used"
        
        # Replay: Buffer should have samples
        assert len(combined.replay_buffer.buffer) > 0, "Replay not used"
        
        # Adapters: Should have task-specific adapters
        assert len(combined.adapters) > 0, "Adapters not created"
    
    def test_combined_vs_individual_strategies(self):
        """Test 10.5: COMBINED outperforms individual strategies."""
        # This is THE KEY TEST for the flagship feature
        
        # Create COMBINED system
        combined = create_continual_learning_system(strategy='combined')
        
        # Create individual strategy systems for comparison
        ewc_only = create_continual_learning_system(strategy='ewc')
        replay_only = create_continual_learning_system(strategy='replay')
        
        # Shared task sequence
        tasks = [
            Task(id=f"t{i}", name=f"Task {i}", num_classes=5, difficulty=0.5)
            for i in range(3)
        ]
        
        # Train all systems
        systems = {'COMBINED': combined, 'EWC': ewc_only, 'Replay': replay_only}
        results = {name: [] for name in systems.keys()}
        
        for task in tasks:
            for name, system in systems.items():
                system.register_task(task)
                
                # Train
                for epoch in range(5):
                    data = torch.randn(32, system.model.input_dim)
                    labels = torch.randint(0, 5, (32,))
                    loss = system.train_step(data, labels, task_id=task.id)
                
                # Evaluate on ALL previous tasks
                avg_performance = []
                for prev_task in tasks[:tasks.index(task) + 1]:
                    perf = system.evaluate(prev_task)
                    avg_performance.append(perf)
                
                results[name].append(np.mean(avg_performance))
        
        # COMBINED should have best average performance
        final_combined = np.mean(results['COMBINED'])
        final_ewc = np.mean(results['EWC'])
        final_replay = np.mean(results['Replay'])
        
        print(f"\n   ğŸ“Š Performance Comparison:")
        print(f"   COMBINED: {final_combined:.3f}")
        print(f"   EWC Only: {final_ewc:.3f}")
        print(f"   Replay Only: {final_replay:.3f}")
        
        # COMBINED should be competitive or better
        assert final_combined >= 0, "COMBINED performance tracking works"


def run_all_tests():
    """Run all COMBINED strategy core tests."""
    print("=" * 80)
    print("ğŸŒŸ PHASE 1 - TEST 10: COMBINED Strategy Core (FLAGSHIP)")
    print("=" * 80)
    
    test_suite = TestCOMBINEDStrategyCore()
    results = {
        'total': 5,
        'passed': 0,
        'failed': 0,
        'errors': []
    }
    
    tests = [
        ('COMBINED Strategy Creation', test_suite.test_combined_strategy_creation),
        ('Adaptive Strategy Selection', test_suite.test_adaptive_strategy_selection),
        ('Interference Detection', test_suite.test_interference_detection_and_mitigation),
        ('Multi-Component Integration', test_suite.test_multi_component_integration),
        ('COMBINED vs Individual Strategies', test_suite.test_combined_vs_individual_strategies),
    ]
    
    for test_name, test_func in tests:
        try:
            print(f"\nğŸ§ª Running: {test_name}...")
            test_func()
            print(f"âœ… PASSED: {test_name}")
            results['passed'] += 1
        except Exception as e:
            print(f"âŒ FAILED: {test_name}")
            print(f"   Error: {str(e)}")
            results['failed'] += 1
            results['errors'].append({
                'test': test_name,
                'error': str(e)
            })
    
    # Print summary
    print("\n" + "=" * 80)
    print("ğŸŒŸ TEST SUMMARY - COMBINED Strategy Core (FLAGSHIP)")
    print("=" * 80)
    print(f"Total Tests: {results['total']}")
    print(f"Passed: {results['passed']}")
    print(f"Failed: {results['failed']}")
    print(f"Success Rate: {(results['passed'] / results['total'] * 100):.1f}%")
    
    if results['passed'] == results['total']:
        print("\nğŸ† FLAGSHIP FEATURE VALIDATED! SymbioAI COMBINED strategy is operational!")
    
    return results


if __name__ == "__main__":
    results = run_all_tests()
    sys.exit(0 if results['failed'] == 0 else 1)
