#!/usr/bin/env python3
"""
Model Fusion and Comparison Experiment - Symbio AI (Demo Version)
Following the exact prompt structure for benchmarking fusion strategies.

This demo version shows the complete experimental framework without ML dependencies.

# Experiment: Compare different model fusion strategies
strategies = {
    "Direct Ensemble": lambda outputs: sum(outputs)/len(outputs),      # average predictions
    "Weighted Average (0.7/0.3)": lambda outs: 0.7*outs[0] + 0.3*outs[1],  # weighted sum
    "Parameter Merging (50/50)": None,  # to be filled by loading merged model weights
}

This experiment empirically answers: "Does weight merging outperform simple ensembling on our data?"
"""

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Callable
import random
import math

# Mock tensor class for demonstration
class MockTensor:
    """Mock PyTorch tensor for demonstration purposes."""
    
    def __init__(self, data, shape=None):
        if isinstance(data, (list, tuple)):
            self.data = np.array(data) if HAS_NUMPY else data
            self.shape = self.data.shape if hasattr(self.data, 'shape') else (len(data),)
        elif isinstance(data, (int, float)):
            self.data = data
            self.shape = shape or ()
        else:
            self.data = data
            self.shape = shape or getattr(data, 'shape', ())
    
    def __add__(self, other):
        if isinstance(other, MockTensor):
            return MockTensor([a + b for a, b in zip(self.data, other.data)])
        return MockTensor([a + other for a in self.data])
    
    def __mul__(self, other):
        if isinstance(other, MockTensor):
            return MockTensor([a * b for a, b in zip(self.data, other.data)])
        return MockTensor([a * other for a in self.data])
    
    def __rmul__(self, other):
        return self.__mul__(other)
    
    def __truediv__(self, other):
        return MockTensor([a / other for a in self.data])
    
    def __len__(self):
        return len(self.data)
    
    def mean(self, dim=None):
        if hasattr(self.data, 'mean'):
            return MockTensor(self.data.mean())
        return MockTensor(sum(self.data) / len(self.data))
    
    def argmax(self, dim=None):
        if hasattr(self.data, 'argmax'):
            return MockTensor(self.data.argmax())
        return MockTensor(self.data.index(max(self.data)))
    
    def round(self):
        return MockTensor([round(x) for x in self.data])
    
    def squeeze(self):
        return self
    
    def unsqueeze(self, dim):
        return self
    
    def long(self):
        return MockTensor([int(x) for x in self.data])
    
    def float(self):
        return MockTensor([float(x) for x in self.data])
    
    def item(self):
        return self.data if isinstance(self.data, (int, float)) else self.data[0]

# Mock torch module
class MockTorch:
    """Mock PyTorch module for demonstration."""
    
    @staticmethod
    def randn(*shape):
        size = 1
        for s in shape:
            size *= s
        data = [random.gauss(0, 1) for _ in range(size)]
        if len(shape) == 1:
            return MockTensor(data)
        return MockTensor(data, shape)
    
    @staticmethod
    def randint(low, high, size):
        if isinstance(size, tuple):
            total_size = 1
            for s in size:
                total_size *= s
            data = [random.randint(low, high-1) for _ in range(total_size)]
        else:
            data = [random.randint(low, high-1) for _ in range(size)]
        return MockTensor(data)
    
    @staticmethod
    def zeros(*shape):
        size = 1
        for s in shape:
            size *= s
        return MockTensor([0.0] * size)
    
    @staticmethod
    def ones(*shape):
        size = 1
        for s in shape:
            size *= s
        return MockTensor([1.0] * size)
    
    @staticmethod
    def arange(n):
        return MockTensor(list(range(n)))
    
    @staticmethod
    def stack(tensors, dim=0):
        return MockTensor([t.data for t in tensors])
    
    @staticmethod
    def max(tensor, dim=None):
        if dim is not None:
            max_vals = [max(tensor.data)]
            indices = [tensor.data.index(max(tensor.data))]
            return (MockTensor(max_vals), MockTensor(indices))
        return MockTensor(max(tensor.data))
    
    @staticmethod
    def topk(tensor, k):
        sorted_data = sorted(enumerate(tensor.data), key=lambda x: x[1], reverse=True)
        indices = [x[0] for x in sorted_data[:k]]
        values = [x[1] for x in sorted_data[:k]]
        return (MockTensor(values), MockTensor(indices))
    
    @staticmethod
    def softmax(tensor, dim=None):
        exp_vals = [math.exp(x - max(tensor.data)) for x in tensor.data]
        sum_exp = sum(exp_vals)
        return MockTensor([x / sum_exp for x in exp_vals])
    
    @staticmethod
    def log(tensor):
        return MockTensor([math.log(max(x, 1e-8)) for x in tensor.data])
    
    @staticmethod
    def bincount(tensor):
        counts = {}
        data_list = tensor.data if isinstance(tensor.data, list) else list(tensor.data)
        for x in data_list:
            counts[x] = counts.get(x, 0) + 1
        max_val = max(data_list) if data_list else 0
        return MockTensor([counts.get(i, 0) for i in range(max_val + 1)])
    
    class no_grad:
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            pass

# Use mock torch
torch = MockTorch()

class MathWordProblemModel:
    """Mock model architecture for mathematical reasoning tasks."""
    
    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, output_dim: int = 10, 
                 num_layers: int = 3, dropout: float = 0.1, model_variant: str = "A"):
        self.model_variant = model_variant
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Mock parameters
        self.parameters = self._create_mock_parameters()
        self._initialize_weights()
    
    def _create_mock_parameters(self):
        """Create mock parameters for the model."""
        params = {}
        
        # Input layer
        params['input_weight'] = torch.randn(self.hidden_dim, self.input_dim)
        params['input_bias'] = torch.randn(self.hidden_dim)
        
        # Hidden layers
        for i in range(self.num_layers - 1):
            if self.model_variant == "A":
                # Model A: Wider networks
                params[f'hidden_{i}_weight_1'] = torch.randn(self.hidden_dim * 2, self.hidden_dim)
                params[f'hidden_{i}_bias_1'] = torch.randn(self.hidden_dim * 2)
                params[f'hidden_{i}_weight_2'] = torch.randn(self.hidden_dim, self.hidden_dim * 2)
                params[f'hidden_{i}_bias_2'] = torch.randn(self.hidden_dim)
            else:
                # Model B: Deeper but narrower
                params[f'hidden_{i}_weight'] = torch.randn(self.hidden_dim, self.hidden_dim)
                params[f'hidden_{i}_bias'] = torch.randn(self.hidden_dim)
        
        # Output layer
        params['output_weight'] = torch.randn(self.output_dim, self.hidden_dim)
        params['output_bias'] = torch.randn(self.output_dim)
        
        return params
    
    def _initialize_weights(self):
        """Initialize weights with different strategies for model variants."""
        # Mock weight initialization - just add some variant-specific noise
        if self.model_variant == "A":
            # Xavier-like initialization for model A
            for key, param in self.parameters.items():
                if hasattr(param, 'data'):
                    param.data = [x * 0.1 for x in param.data]
        else:
            # Kaiming-like initialization for model B
            for key, param in self.parameters.items():
                if hasattr(param, 'data'):
                    param.data = [x * 0.2 for x in param.data]
    
    def __call__(self, x):
        return self.forward(x)
    
    def forward(self, x):
        """Forward pass simulation."""
        batch_size = len(x.data) if hasattr(x.data, '__len__') else 1
        
        # Simple linear transformation simulation
        # In reality, this would involve matrix multiplication through layers
        
        # Add some model-variant specific behavior
        if self.model_variant == "A":
            # Model A tends to be more confident
            confidence_boost = 0.1
        elif self.model_variant == "B":
            # Model B is more conservative
            confidence_boost = -0.05
        else:
            # Merged model
            confidence_boost = 0.025
        
        # Simulate output logits based on input patterns
        outputs = []
        for i in range(batch_size):
            # Extract features from input (simplified)
            if hasattr(x.data[0], '__len__'):
                input_features = x.data[i][:50] if len(x.data[i]) >= 50 else x.data[i]
            else:
                input_features = [x.data[i % len(x.data)]]
            
            # Simulate prediction based on input
            feature_sum = sum(input_features[:10]) if len(input_features) >= 10 else sum(input_features)
            base_prediction = abs(feature_sum) % self.output_dim
            
            # Create logits with some noise
            logits = [random.gauss(0, 0.5) for _ in range(self.output_dim)]
            logits[int(base_prediction)] += 2.0 + confidence_boost  # Make correct class more likely
            
            outputs.append(logits)
        
        return MockTensor(outputs)
    
    def eval(self):
        """Set model to evaluation mode."""
        pass
    
    def state_dict(self):
        """Return model parameters."""
        return self.parameters.copy()
    
    def load_state_dict(self, state_dict, strict=True):
        """Load model parameters."""
        self.parameters.update(state_dict)
    
    def get_model_info(self):
        total_params = sum(len(p.data) if hasattr(p.data, '__len__') else 1 
                          for p in self.parameters.values())
        return {
            "variant": self.model_variant,
            "total_parameters": total_params,
            "input_dim": self.input_dim,
            "hidden_dim": self.hidden_dim,
            "output_dim": self.output_dim
        }


def load_sample_inputs(task: str = "math_word_problems", batch_size: int = 32, input_dim: int = 512):
    """
    Generate sample inputs for mathematical word problems.
    In a real scenario, this would load actual preprocessed text embeddings.
    """
    if task == "math_word_problems":
        # Simulate mathematical problem embeddings with realistic patterns
        inputs = []
        
        for i in range(batch_size):
            # Create realistic input patterns
            input_vec = [random.gauss(0, 0.1) for _ in range(input_dim)]
            
            # Add mathematical structure
            for j in range(50):
                input_vec[j] = random.gauss(0, 0.5) + j * 0.01
            
            # Operation indicators
            op_type = i % 4
            for j in range(51, 100):
                if j >= 51 + op_type * 12 and j < 51 + (op_type + 1) * 12:
                    input_vec[j] = 1.0
                else:
                    input_vec[j] = 0.0
            
            # Context and language features
            for j in range(101, 200):
                input_vec[j] = random.gauss(0, 0.3)
            
            for j in range(200, input_dim):
                input_vec[j] = random.gauss(0, 0.2)
            
            inputs.append(input_vec)
        
    else:
        inputs = [[random.gauss(0, 1) for _ in range(input_dim)] for _ in range(batch_size)]
    
    return MockTensor(inputs)


def generate_ground_truth(inputs, task: str = "math_word_problems"):
    """Generate ground truth labels for evaluation."""
    batch_size = len(inputs.data)
    
    if task == "math_word_problems":
        labels = []
        for i in range(batch_size):
            # Generate labels based on input patterns
            input_row = inputs.data[i]
            number_features = sum(input_row[:50]) / 50
            operation_features = max(range(51, 100), key=lambda j: input_row[j] if j < len(input_row) else 0) - 51
            label = int((number_features * 10 + operation_features) % 10)
            labels.append(label)
    else:
        labels = [random.randint(0, 9) for _ in range(batch_size)]
    
    return MockTensor(labels)


def evaluate_output(predictions, targets=None, task: str = "math_word_problems") -> Dict[str, float]:
    """Evaluate model predictions against ground truth."""
    if targets is None:
        batch_size = len(predictions.data)
        targets = MockTensor([random.randint(0, 9) for _ in range(batch_size)])
    
    # Convert logits to predictions
    if hasattr(predictions.data[0], '__len__') and len(predictions.data[0]) > 1:
        # Multi-class predictions
        pred_classes = []
        probabilities = []
        
        for logits in predictions.data:
            # Convert to list if numpy array
            logits_list = list(logits) if hasattr(logits, '__iter__') else [logits]
            
            # Apply softmax
            max_logit = max(logits_list)
            exp_logits = [math.exp(x - max_logit) for x in logits_list]
            sum_exp = sum(exp_logits)
            probs = [x / sum_exp for x in exp_logits]
            
            pred_class = logits_list.index(max(logits_list))
            pred_classes.append(pred_class)
            probabilities.append(probs)
        
        pred_classes_tensor = MockTensor(pred_classes)
        
        # Calculate metrics
        target_data = targets.data if hasattr(targets, 'data') else targets
        if not hasattr(target_data, '__iter__') or isinstance(target_data, str):
            target_data = [target_data]
            
        correct = sum(1 for p, t in zip(pred_classes, target_data) if p == t)
        accuracy = correct / len(target_data)
        
        confidences = [max(probs) for probs in probabilities]
        avg_confidence = sum(confidences) / len(confidences)
        
        # Calculate entropy
        entropy_sum = 0
        for probs in probabilities:
            entropy_sum += -sum(p * math.log(max(p, 1e-8)) for p in probs)
        entropy = entropy_sum / len(probabilities)
        
        # Top-2 accuracy
        top_2_correct = 0
        for logits, target in zip(predictions.data, target_data):
            logits_list = list(logits) if hasattr(logits, '__iter__') else [logits]
            top_2_indices = sorted(range(len(logits_list)), key=lambda i: logits_list[i], reverse=True)[:2]
            if target in top_2_indices:
                top_2_correct += 1
        top_2_acc = top_2_correct / len(target_data)
        
    else:
        # Single value predictions
        if hasattr(predictions.data, '__iter__') and not isinstance(predictions.data, str):
            try:
                pred_classes = [int(round(float(x))) for x in predictions.data]
            except (ValueError, TypeError):
                # Handle numpy arrays or nested structures
                pred_classes = []
                for x in predictions.data:
                    if hasattr(x, '__iter__') and not isinstance(x, str):
                        # If x is iterable (like a numpy array), take the first element or average
                        pred_classes.append(int(round(float(x[0]) if len(x) > 0 else 0)))
                    else:
                        pred_classes.append(int(round(float(x))))
        else:
            pred_classes = [int(round(float(predictions.data)))]
        pred_classes_tensor = MockTensor(pred_classes)
        
        target_data = targets.data if hasattr(targets, 'data') else targets
        if not hasattr(target_data, '__iter__') or isinstance(target_data, str):
            target_data = [target_data]
        
        correct = sum(1 for p, t in zip(pred_classes, target_data) if p == t)
        accuracy = correct / len(target_data)
        avg_confidence = 1.0
        entropy = 0.0
        top_2_acc = accuracy
    
    # Make sure target_data is available for sample_size
    if 'target_data' not in locals():
        target_data = targets.data if hasattr(targets, 'data') else targets
        if not hasattr(target_data, '__iter__') or isinstance(target_data, str):
            target_data = [target_data]
    
    return {
        "accuracy": accuracy,
        "confidence": avg_confidence,
        "entropy": entropy,
        "top_2_accuracy": top_2_acc,
        "sample_size": len(target_data)
    }


def merge_models(model_a: MathWordProblemModel, model_b: MathWordProblemModel, alpha: float = 0.5) -> MathWordProblemModel:
    """
    Merge two models using linear interpolation of parameters.
    Following the exact prompt: merge_models(model_A, model_B, alpha=0.5)
    """
    merged_model = MathWordProblemModel(
        input_dim=model_a.input_dim,
        hidden_dim=model_a.hidden_dim,
        output_dim=model_a.output_dim,
        model_variant="Merged"
    )
    
    # Merge parameters
    merged_params = {}
    model_a_params = model_a.state_dict()
    model_b_params = model_b.state_dict()
    
    for key in model_a_params.keys():
        if key in model_b_params:
            # Linear interpolation of parameters
            param_a = model_a_params[key]
            param_b = model_b_params[key]
            
            if hasattr(param_a, 'data') and hasattr(param_b, 'data'):
                merged_data = [alpha * a + (1 - alpha) * b 
                             for a, b in zip(param_a.data, param_b.data)]
                merged_params[key] = MockTensor(merged_data)
            else:
                merged_params[key] = param_a
        else:
            merged_params[key] = model_a_params[key]
    
    # Add any parameters that only exist in model_b
    for key in model_b_params.keys():
        if key not in merged_params:
            merged_params[key] = model_b_params[key]
    
    merged_model.load_state_dict(merged_params)
    merged_model.eval()
    
    return merged_model


def run_fusion_experiment():
    """
    Main experiment function following the exact prompt structure.
    """
    print("üî¨ Model Fusion and Comparison Experiment - Symbio AI (Demo)")
    print("=" * 65)
    
    # Experiment: Compare different model fusion strategies
    strategies = {
        "Direct Ensemble": lambda outputs: MockTensor([
            [(o1[i] + o2[i]) / 2 for i in range(len(o1))] 
            for o1, o2 in zip(outputs[0].data, outputs[1].data)
        ]),      # average predictions
        "Weighted Average (0.7/0.3)": lambda outs: 0.7*outs[0] + 0.3*outs[1],  # weighted sum
        "Parameter Merging (50/50)": None,  # to be filled by loading merged model weights
    }
    
    print("üìã Fusion Strategies Defined:")
    for name, func in strategies.items():
        print(f"  ‚Ä¢ {name}: {'Function-based' if func else 'Parameter-level'} fusion")
    
    # Additional strategies for comprehensive analysis
    advanced_strategies = {
        "Weighted Average (0.8/0.2)": lambda outs: 0.8*outs[0] + 0.2*outs[1],
        "Weighted Average (0.6/0.4)": lambda outs: 0.6*outs[0] + 0.4*outs[1],
        "Max Voting": lambda outs: MockTensor([
            [max(o1[i], o2[i]) for i in range(len(o1))] 
            for o1, o2 in zip(outs[0].data, outs[1].data)
        ]),
        "Parameter Merging (30/70)": None,
        "Parameter Merging (70/30)": None,
    }
    
    all_strategies = {**strategies, **advanced_strategies}
    
    # Load or define two base models (e.g., model_A, model_B)
    print("\nüèóÔ∏è Creating base models...")
    
    # Following the prompt structure but with mock models
    # In production: model_A = load_model("checkpointA.pt")
    # In production: model_B = load_model("checkpointB.pt")
    model_A = MathWordProblemModel(input_dim=512, hidden_dim=256, output_dim=10, model_variant="A")
    model_B = MathWordProblemModel(input_dim=512, hidden_dim=256, output_dim=10, model_variant="B")
    
    model_A.eval()
    model_B.eval()
    
    print(f"‚úÖ Model A: {model_A.get_model_info()['total_parameters']:,} parameters")
    print(f"‚úÖ Model B: {model_B.get_model_info()['total_parameters']:,} parameters")
    
    # Create merged models with different ratios
    print("\n‚öôÔ∏è Creating parameter-merged models...")
    merged_models = {}
    merge_ratios = [0.5, 0.3, 0.7]  # 50/50, 30/70, 70/30
    
    for alpha in merge_ratios:
        # Following prompt: merged_model = merge_models(model_A, model_B, alpha=0.5)
        merged_model = merge_models(model_A, model_B, alpha=alpha)
        ratio_str = f"({int(alpha*100)}/{int((1-alpha)*100)})"
        strategy_name = f"Parameter Merging {ratio_str}"
        merged_models[strategy_name] = merged_model
        print(f"  ‚úÖ Created {strategy_name}")
    
    # Sample evaluation on a task
    print("\nüìä Loading sample data...")
    
    # Following prompt structure exactly:
    # inputs = load_sample_inputs(task="math_word_problems")
    inputs = load_sample_inputs(task="math_word_problems", batch_size=64)
    targets = generate_ground_truth(inputs, task="math_word_problems")
    
    print(f"‚úÖ Sample size: {len(inputs)}")
    print(f"‚úÖ Input shape: {inputs.shape}")
    print(f"‚úÖ Target distribution: {torch.bincount(targets)}")
    
    # Get model outputs
    print("\nüîç Evaluating individual models...")
    with torch.no_grad():
        outputs_A = model_A(inputs)
        outputs_B = model_B(inputs)
    
    # Following prompt: outputs_ens = [outputs_A, outputs_B]
    outputs_ens = [outputs_A, outputs_B]
    
    # Evaluate individual performance
    results_A = evaluate_output(outputs_A, targets, task="math_word_problems")
    results_B = evaluate_output(outputs_B, targets, task="math_word_problems")
    
    print(f"Model A Accuracy: {results_A['accuracy']:.4f}")
    print(f"Model B Accuracy: {results_B['accuracy']:.4f}")
    
    # Main evaluation loop - Following exact prompt structure
    print("\nüß™ Running fusion strategy evaluation...")
    print("=" * 50)
    
    # Following prompt structure exactly:
    results = {}
    timing_results = {}
    detailed_metrics = {}
    
    for name, func in all_strategies.items():
        print(f"\nüî¨ Evaluating: {name}")
        start_time = time.time()
        
        try:
            with torch.no_grad():
                if name == "Parameter Merging (50/50)":
                    # Following prompt: merged_out = merged_model(inputs)
                    merged_out = merged_models[name](inputs)
                    # Following prompt: results[name] = evaluate_output(merged_out, task="math_word_problems")
                    evaluation_result = evaluate_output(merged_out, targets, task="math_word_problems")
                elif "Parameter Merging" in name and name in merged_models:
                    merged_out = merged_models[name](inputs)
                    evaluation_result = evaluate_output(merged_out, targets, task="math_word_problems")
                else:
                    if func is not None:
                        # Following prompt: combined = func(outputs_ens)
                        combined = func(outputs_ens)
                        # Following prompt: results[name] = evaluate_output(combined, task="math_word_problems")
                        evaluation_result = evaluate_output(combined, targets, task="math_word_problems")
                    else:
                        print(f"  ‚ö†Ô∏è Function not implemented for {name}")
                        continue
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # Store results
            results[name] = evaluation_result['accuracy']
            timing_results[name] = execution_time
            detailed_metrics[name] = evaluation_result
            
            print(f"  ‚úÖ Accuracy: {evaluation_result['accuracy']:.4f}")
            print(f"  ‚è±Ô∏è Time: {execution_time:.6f}s")
            
        except Exception as e:
            print(f"  ‚ùå Error: {str(e)}")
            continue
    
    # Following prompt: print(results)
    print("\nüìä FUSION STRATEGY RESULTS:")
    print("=" * 40)
    
    # Sort and display results
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    
    for rank, (strategy, accuracy) in enumerate(sorted_results, 1):
        timing = timing_results.get(strategy, 0)
        print(f"{rank:2d}. {strategy:<35} | Acc: {accuracy:.4f} | Time: {timing:.4f}s")
    
    # Analysis and insights
    print("\nüéØ KEY INSIGHTS:")
    print("-" * 25)
    
    best_strategy = sorted_results[0]
    best_individual = max(results_A['accuracy'], results_B['accuracy'])
    improvement = best_strategy[1] - best_individual
    
    print(f"üèÜ Best Strategy: {best_strategy[0]} ({best_strategy[1]:.4f})")
    print(f"üìà Improvement over best individual: {improvement:.4f} ({improvement*100:.2f}%)")
    
    # Answer the key question from the prompt
    param_merging_results = {k: v for k, v in results.items() if 'Parameter Merging' in k}
    output_fusion_results = {k: v for k, v in results.items() if 'Parameter Merging' not in k}
    
    if param_merging_results and output_fusion_results:
        best_param = max(param_merging_results.values())
        best_output = max(output_fusion_results.values())
        
        print("\nü§î Key Question: 'Does weight merging outperform simple ensembling?'")
        if best_param > best_output:
            print(f"‚úÖ YES! Parameter merging ({best_param:.4f}) > Output fusion ({best_output:.4f})")
            print(f"   Advantage: {best_param - best_output:.4f} ({(best_param/best_output-1)*100:.2f}%)")
        else:
            print(f"‚ùå NO. Output fusion ({best_output:.4f}) > Parameter merging ({best_param:.4f})")
            print(f"   Advantage: {best_output - best_param:.4f} ({(best_output/best_param-1)*100:.2f}%)")
    
    # Save results
    experiment_summary = {
        "strategies_tested": len(results),
        "best_strategy": best_strategy[0],
        "best_accuracy": best_strategy[1],
        "improvement_over_individual": improvement,
        "individual_performance": {
            "model_a": results_A['accuracy'],
            "model_b": results_B['accuracy']
        },
        "all_results": results,
        "timing_results": timing_results
    }
    
    # Create results directory
    results_dir = Path(__file__).parent
    results_file = results_dir / "fusion_experiment_results_demo.json"
    
    with open(results_file, 'w') as f:
        json.dump(experiment_summary, f, indent=2, default=str)
    
    print(f"\nüíæ Results saved to: {results_file}")
    print(f"\nüéâ Experiment Complete! Tested {len(results)} fusion strategies")
    
    return results, experiment_summary


if __name__ == "__main__":
    # Set random seed for reproducibility
    random.seed(42)
    
    # Run the fusion experiment
    results, summary = run_fusion_experiment()
    
    print("\n" + "="*60)
    print("‚úÖ MODEL FUSION EXPERIMENT SUCCESSFUL (DEMO)")
    print("="*60)
    print(f"üéØ Primary finding: {summary['best_strategy']} achieved {summary['best_accuracy']:.4f} accuracy")
    print(f"üìà Performance gain: {summary['improvement_over_individual']*100:.2f}% over best individual model")
    print("üöÄ Ready for MVP implementation with optimal fusion strategy!")
    print("\nüí° This demo shows the complete experimental framework.")
    print("   Replace mock implementations with actual PyTorch models for production use.")