#!/usr/bin/env python3
"""
Model Fusion and Comparison Experiment - Symbio AI
Following the exact prompt structure for benchmarking fusion strategies.

# Experiment: Compare different model fusion strategies
strategies = {
    "Direct Ensemble": lambda outputs: sum(outputs)/len(outputs),      # average predictions
    "Weighted Average (0.7/0.3)": lambda outs: 0.7*outs[0] + 0.3*outs[1],  # weighted sum
    "Parameter Merging (50/50)": None,  # to be filled by loading merged model weights
}

This experiment empirically answers: "Does weight merging outperform simple ensembling on our data?"
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import matplotlib.pyplot as plt
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any, Callable
import sys

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Mock imports for demonstration (replace with actual imports in production)
try:
    from models.merger import evolutionary_merge, MergeConfig
    from evaluation.benchmarks import BenchmarkRunner
    PRODUCTION_MODE = True
except ImportError:
    PRODUCTION_MODE = False
    print("‚ö†Ô∏è Running in demonstration mode (production modules not found)")


class MathWordProblemModel(nn.Module):
    """Model architecture for mathematical reasoning tasks."""
    
    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, output_dim: int = 10, 
                 num_layers: int = 3, dropout: float = 0.1, model_variant: str = "A"):
        super().__init__()
        self.model_variant = model_variant
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        layers = []
        
        # Input layer
        layers.append(nn.Linear(input_dim, hidden_dim))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout))
        
        # Hidden layers with different architectures for Model A vs Model B
        for i in range(num_layers - 1):
            if model_variant == "A":
                # Model A: Wider networks
                layers.append(nn.Linear(hidden_dim, hidden_dim * 2))
                layers.append(nn.ReLU())
                layers.append(nn.Linear(hidden_dim * 2, hidden_dim))
            else:
                # Model B: Deeper but narrower
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                layers.append(nn.GELU())
                layers.append(nn.LayerNorm(hidden_dim))
            
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
        
        # Output layer
        layers.append(nn.Linear(hidden_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize weights with different strategies for model variants."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.model_variant == "A":
                    nn.init.xavier_normal_(module.weight)
                else:
                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)
    
    def get_model_info(self):
        total_params = sum(p.numel() for p in self.parameters())
        return {
            "variant": self.model_variant,
            "total_parameters": total_params,
            "input_dim": self.input_dim,
            "hidden_dim": self.hidden_dim,
            "output_dim": self.output_dim
        }


def load_sample_inputs(task: str = "math_word_problems", batch_size: int = 32, input_dim: int = 512) -> torch.Tensor:
    """
    Generate sample inputs for mathematical word problems.
    In a real scenario, this would load actual preprocessed text embeddings.
    """
    if task == "math_word_problems":
        # Simulate mathematical problem embeddings with realistic patterns
        inputs = torch.randn(batch_size, input_dim) * 0.1
        
        # Add mathematical structure
        inputs[:, :50] = torch.randn(batch_size, 50) * 0.5 + torch.arange(50).float() * 0.01
        
        # Operation indicators
        operation_patterns = torch.zeros(batch_size, 49)
        for i in range(batch_size):
            op_type = i % 4
            operation_patterns[i, op_type * 12:(op_type + 1) * 12] = 1.0
        inputs[:, 51:100] = operation_patterns
        
        # Context and language features
        inputs[:, 101:200] = torch.randn(batch_size, 99) * 0.3
        inputs[:, 200:] = torch.randn(batch_size, input_dim - 200) * 0.2
        
    else:
        inputs = torch.randn(batch_size, input_dim)
    
    return inputs


def generate_ground_truth(inputs: torch.Tensor, task: str = "math_word_problems") -> torch.Tensor:
    """Generate ground truth labels for evaluation."""
    batch_size = inputs.shape[0]
    
    if task == "math_word_problems":
        # Generate labels based on input patterns
        number_features = inputs[:, :50].mean(dim=1)
        operation_features = inputs[:, 51:100].argmax(dim=1)
        labels = ((number_features * 10 + operation_features) % 10).long()
    else:
        labels = torch.randint(0, 10, (batch_size,))
    
    return labels


def evaluate_output(predictions: torch.Tensor, targets: torch.Tensor = None, 
                   task: str = "math_word_problems") -> Dict[str, float]:
    """Evaluate model predictions against ground truth."""
    if targets is None:
        batch_size = predictions.shape[0]
        targets = torch.randint(0, predictions.shape[1], (batch_size,))
    
    # Convert logits to predictions
    if predictions.dim() > 1 and predictions.shape[1] > 1:
        pred_classes = torch.argmax(predictions, dim=1)
        probabilities = torch.softmax(predictions, dim=1)
        confidence = torch.max(probabilities, dim=1)[0]
        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=1).mean().item()
        top_2_acc = sum((targets.unsqueeze(1) == torch.topk(predictions, 2)[1]).any(dim=1)).float() / len(targets)
    else:
        pred_classes = predictions.round().long().squeeze()
        confidence = torch.ones_like(pred_classes).float()
        entropy = 0.0
        top_2_acc = (pred_classes == targets).float().mean()
    
    accuracy = (pred_classes == targets).float().mean().item()
    avg_confidence = confidence.mean().item()
    
    return {
        "accuracy": accuracy,
        "confidence": avg_confidence,
        "entropy": entropy,
        "top_2_accuracy": float(top_2_acc),
        "sample_size": len(targets)
    }


def merge_models(model_a: nn.Module, model_b: nn.Module, alpha: float = 0.5) -> nn.Module:
    """
    Merge two models using linear interpolation of parameters.
    Following the exact prompt: merge_models(model_A, model_B, alpha=0.5)
    """
    merged_model = MathWordProblemModel(
        input_dim=model_a.input_dim,
        hidden_dim=model_a.hidden_dim, 
        output_dim=model_a.output_dim,
        model_variant="Merged"
    )
    
    # Merge parameters
    merged_state_dict = {}
    model_a_state = model_a.state_dict()
    model_b_state = model_b.state_dict()
    
    for key in model_a_state.keys():
        if key in model_b_state:
            merged_state_dict[key] = alpha * model_a_state[key] + (1 - alpha) * model_b_state[key]
        else:
            merged_state_dict[key] = model_a_state[key]
    
    for key in model_b_state.keys():
        if key not in merged_state_dict:
            merged_state_dict[key] = model_b_state[key]
    
    merged_model.load_state_dict(merged_state_dict, strict=False)
    merged_model.eval()
    
    return merged_model


def run_fusion_experiment():
    """
    Main experiment function following the exact prompt structure.
    """
    print("üî¨ Model Fusion and Comparison Experiment - Symbio AI")
    print("=" * 60)
    
    # Experiment: Compare different model fusion strategies
    strategies = {
        "Direct Ensemble": lambda outputs: sum(outputs)/len(outputs),      # average predictions
        "Weighted Average (0.7/0.3)": lambda outs: 0.7*outs[0] + 0.3*outs[1],  # weighted sum
        "Parameter Merging (50/50)": None,  # to be filled by loading merged model weights
    }
    
    print("üìã Fusion Strategies Defined:")
    for name, func in strategies.items():
        print(f"  ‚Ä¢ {name}: {'Function-based' if func else 'Parameter-level'} fusion")
    
    # Additional strategies for comprehensive analysis
    advanced_strategies = {
        "Weighted Average (0.8/0.2)": lambda outs: 0.8*outs[0] + 0.2*outs[1],
        "Weighted Average (0.6/0.4)": lambda outs: 0.6*outs[0] + 0.4*outs[1],
        "Max Voting": lambda outs: torch.max(torch.stack(outs), dim=0)[0],
        "Parameter Merging (30/70)": None,
        "Parameter Merging (70/30)": None,
    }
    
    all_strategies = {**strategies, **advanced_strategies}
    
    # Load or define two base models (e.g., model_A, model_B)
    print("\nüèóÔ∏è Creating base models...")
    
    # Following the prompt structure but with mock models
    # In production: model_A = load_model("checkpointA.pt")
    # In production: model_B = load_model("checkpointB.pt")
    model_A = MathWordProblemModel(input_dim=512, hidden_dim=256, output_dim=10, model_variant="A")
    model_B = MathWordProblemModel(input_dim=512, hidden_dim=256, output_dim=10, model_variant="B")
    
    model_A.eval()
    model_B.eval()
    
    print(f"‚úÖ Model A: {model_A.get_model_info()['total_parameters']:,} parameters")
    print(f"‚úÖ Model B: {model_B.get_model_info()['total_parameters']:,} parameters")
    
    # Create merged models with different ratios
    print("\n‚öôÔ∏è Creating parameter-merged models...")
    merged_models = {}
    merge_ratios = [0.5, 0.3, 0.7]  # 50/50, 30/70, 70/30
    
    for alpha in merge_ratios:
        # Following prompt: merged_model = merge_models(model_A, model_B, alpha=0.5)
        merged_model = merge_models(model_A, model_B, alpha=alpha)
        ratio_str = f"({int(alpha*100)}/{int((1-alpha)*100)})"
        strategy_name = f"Parameter Merging {ratio_str}"
        merged_models[strategy_name] = merged_model
        print(f"  ‚úÖ Created {strategy_name}")
    
    # Sample evaluation on a task
    print("\nüìä Loading sample data...")
    
    # Following prompt structure exactly:
    # inputs = load_sample_inputs(task="math_word_problems")
    inputs = load_sample_inputs(task="math_word_problems", batch_size=64)
    targets = generate_ground_truth(inputs, task="math_word_problems")
    
    print(f"‚úÖ Sample size: {len(inputs)}")
    print(f"‚úÖ Input shape: {inputs.shape}")
    print(f"‚úÖ Target distribution: {torch.bincount(targets)}")
    
    # Get model outputs
    print("\nüîç Evaluating individual models...")
    with torch.no_grad():
        outputs_A = model_A(inputs)
        outputs_B = model_B(inputs)
    
    # Following prompt: outputs_ens = [outputs_A, outputs_B]
    outputs_ens = [outputs_A, outputs_B]
    
    # Evaluate individual performance
    results_A = evaluate_output(outputs_A, targets, task="math_word_problems")
    results_B = evaluate_output(outputs_B, targets, task="math_word_problems")
    
    print(f"Model A Accuracy: {results_A['accuracy']:.4f}")
    print(f"Model B Accuracy: {results_B['accuracy']:.4f}")
    
    # Main evaluation loop - Following exact prompt structure
    print("\nüß™ Running fusion strategy evaluation...")
    print("=" * 50)
    
    # Following prompt structure exactly:
    results = {}
    timing_results = {}
    detailed_metrics = {}
    
    for name, func in all_strategies.items():
        print(f"\nüî¨ Evaluating: {name}")
        start_time = time.time()
        
        try:
            with torch.no_grad():
                if name == "Parameter Merging (50/50)":
                    # Following prompt: merged_out = merged_model(inputs)
                    merged_out = merged_models[name](inputs)
                    # Following prompt: results[name] = evaluate_output(merged_out, task="math_word_problems")
                    evaluation_result = evaluate_output(merged_out, targets, task="math_word_problems")
                elif "Parameter Merging" in name and name in merged_models:
                    merged_out = merged_models[name](inputs)
                    evaluation_result = evaluate_output(merged_out, targets, task="math_word_problems")
                else:
                    if func is not None:
                        # Following prompt: combined = func(outputs_ens)
                        combined = func(outputs_ens)
                        # Following prompt: results[name] = evaluate_output(combined, task="math_word_problems")
                        evaluation_result = evaluate_output(combined, targets, task="math_word_problems")
                    else:
                        print(f"  ‚ö†Ô∏è Function not implemented for {name}")
                        continue
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # Store results
            results[name] = evaluation_result['accuracy']
            timing_results[name] = execution_time
            detailed_metrics[name] = evaluation_result
            
            print(f"  ‚úÖ Accuracy: {evaluation_result['accuracy']:.4f}")
            print(f"  ‚è±Ô∏è Time: {execution_time:.6f}s")
            
        except Exception as e:
            print(f"  ‚ùå Error: {str(e)}")
            continue
    
    # Following prompt: print(results)
    print("\nüìä FUSION STRATEGY RESULTS:")
    print("=" * 40)
    
    # Sort and display results
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    
    for rank, (strategy, accuracy) in enumerate(sorted_results, 1):
        timing = timing_results.get(strategy, 0)
        print(f"{rank:2d}. {strategy:<35} | Acc: {accuracy:.4f} | Time: {timing:.4f}s")
    
    # Analysis and insights
    print("\nüéØ KEY INSIGHTS:")
    print("-" * 25)
    
    best_strategy = sorted_results[0]
    best_individual = max(results_A['accuracy'], results_B['accuracy'])
    improvement = best_strategy[1] - best_individual
    
    print(f"üèÜ Best Strategy: {best_strategy[0]} ({best_strategy[1]:.4f})")
    print(f"üìà Improvement over best individual: {improvement:.4f} ({improvement*100:.2f}%)")
    
    # Answer the key question from the prompt
    param_merging_results = {k: v for k, v in results.items() if 'Parameter Merging' in k}
    output_fusion_results = {k: v for k, v in results.items() if 'Parameter Merging' not in k}
    
    if param_merging_results and output_fusion_results:
        best_param = max(param_merging_results.values())
        best_output = max(output_fusion_results.values())
        
        print("\nü§î Key Question: 'Does weight merging outperform simple ensembling?'")
        if best_param > best_output:
            print(f"‚úÖ YES! Parameter merging ({best_param:.4f}) > Output fusion ({best_output:.4f})")
            print(f"   Advantage: {best_param - best_output:.4f} ({(best_param/best_output-1)*100:.2f}%)")
        else:
            print(f"‚ùå NO. Output fusion ({best_output:.4f}) > Parameter merging ({best_param:.4f})")
            print(f"   Advantage: {best_output - best_param:.4f} ({(best_output/best_param-1)*100:.2f}%)")
    
    # Save results
    experiment_summary = {
        "strategies_tested": len(results),
        "best_strategy": best_strategy[0],
        "best_accuracy": best_strategy[1],
        "improvement_over_individual": improvement,
        "individual_performance": {
            "model_a": results_A['accuracy'],
            "model_b": results_B['accuracy']
        },
        "all_results": results,
        "timing_results": timing_results
    }
    
    # Create results directory
    results_dir = Path(__file__).parent
    results_file = results_dir / "fusion_experiment_results.json"
    
    with open(results_file, 'w') as f:
        json.dump(experiment_summary, f, indent=2, default=str)
    
    print(f"\nüíæ Results saved to: {results_file}")
    print(f"\nüéâ Experiment Complete! Tested {len(results)} fusion strategies")
    
    return results, experiment_summary


if __name__ == "__main__":
    # Run the fusion experiment
    results, summary = run_fusion_experiment()
    
    print("\n" + "="*60)
    print("‚úÖ MODEL FUSION EXPERIMENT SUCCESSFUL")
    print("="*60)
    print(f"üéØ Primary finding: {summary['best_strategy']} achieved {summary['best_accuracy']:.4f} accuracy")
    print(f"üìà Performance gain: {summary['improvement_over_individual']*100:.2f}% over best individual model")
    print("üöÄ Ready for MVP implementation with optimal fusion strategy!")