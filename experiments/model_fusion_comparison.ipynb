{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3827d6",
   "metadata": {},
   "source": [
    "# Model Fusion and Comparison Experiment\n",
    "\n",
    "## Objective\n",
    "\n",
    "Benchmark different model fusion strategies to empirically answer:\n",
    "- **\"Does weight merging outperform simple ensembling on our data?\"**\n",
    "- **\"Which fusion strategy yields the best performance for our MVP?\"**\n",
    "\n",
    "## Strategies Evaluated\n",
    "\n",
    "1. **Direct Ensemble**: Simple output averaging\n",
    "2. **Weighted Average**: Optimized weighted combination\n",
    "3. **Parameter Merging**: Direct model weight fusion\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- Performance comparison across fusion strategies\n",
    "- Computational efficiency analysis\n",
    "- Recommendations for optimal model combination approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dadfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any, Callable\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent if Path().resolve().name == 'experiments' else Path().resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import Symbio AI components\n",
    "from models.merger import evolutionary_merge, MergeConfig, EvolutionaryModelMerger\n",
    "from evaluation.benchmarks import BenchmarkRunner, AccuracyBenchmark\n",
    "from monitoring.production import MetricsCollector, ProductionLogger\n",
    "\n",
    "print(\"✅ Dependencies loaded successfully\")\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🐍 Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f4061",
   "metadata": {},
   "source": [
    "## Experiment Setup: Model Fusion Strategies\n",
    "\n",
    "Following the exact prompt specification, we define multiple fusion approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Compare different model fusion strategies\n",
    "strategies = {\n",
    "    \"Direct Ensemble\": lambda outputs: sum(outputs)/len(outputs),      # average predictions\n",
    "    \"Weighted Average (0.7/0.3)\": lambda outs: 0.7*outs[0] + 0.3*outs[1],  # weighted sum\n",
    "    \"Parameter Merging (50/50)\": None,  # to be filled by loading merged model weights\n",
    "}\n",
    "\n",
    "print(\"🔬 Model Fusion Strategies Defined:\")\n",
    "for name, func in strategies.items():\n",
    "    if func is not None:\n",
    "        print(f\"  ✓ {name}: Function-based fusion\")\n",
    "    else:\n",
    "        print(f\"  ⚙️ {name}: Parameter-level fusion (to be implemented)\")\n",
    "\n",
    "# Additional advanced strategies for comprehensive comparison\n",
    "advanced_strategies = {\n",
    "    \"Weighted Average (0.8/0.2)\": lambda outs: 0.8*outs[0] + 0.2*outs[1],\n",
    "    \"Weighted Average (0.6/0.4)\": lambda outs: 0.6*outs[0] + 0.4*outs[1],\n",
    "    \"Max Voting\": lambda outs: torch.max(torch.stack(outs), dim=0)[0],\n",
    "    \"Geometric Mean\": lambda outs: torch.exp(torch.mean(torch.log(torch.stack(outs) + 1e-8), dim=0)),\n",
    "    \"Parameter Merging (30/70)\": None,\n",
    "    \"Parameter Merging (70/30)\": None,\n",
    "}\n",
    "\n",
    "# Combine strategies\n",
    "all_strategies = {**strategies, **advanced_strategies}\n",
    "\n",
    "print(f\"\\n📊 Total strategies to evaluate: {len(all_strategies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caafb3f",
   "metadata": {},
   "source": [
    "## Model Architecture Definition\n",
    "\n",
    "Create two base models with different architectures for realistic fusion testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathWordProblemModel(nn.Module):\n",
    "    \"\"\"Model architecture optimized for mathematical reasoning tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, output_dim: int = 10, \n",
    "                 num_layers: int = 3, dropout: float = 0.1, model_variant: str = \"A\"):\n",
    "        super().__init__()\n",
    "        self.model_variant = model_variant\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Hidden layers with different architectures for Model A vs Model B\n",
    "        for i in range(num_layers - 1):\n",
    "            if model_variant == \"A\":\n",
    "                # Model A: Wider networks\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim * 2))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Linear(hidden_dim * 2, hidden_dim))\n",
    "            else:\n",
    "                # Model B: Deeper but narrower\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.GELU())  # Different activation\n",
    "                layers.append(nn.LayerNorm(hidden_dim))  # Add normalization\n",
    "            \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights differently for each variant\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with different strategies for model variants.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if self.model_variant == \"A\":\n",
    "                    # Xavier initialization for Model A\n",
    "                    nn.init.xavier_normal_(module.weight)\n",
    "                else:\n",
    "                    # He initialization for Model B\n",
    "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model architecture information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            \"variant\": self.model_variant,\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_dim\": self.hidden_dim,\n",
    "            \"output_dim\": self.output_dim\n",
    "        }\n",
    "\n",
    "# Create model instances\n",
    "print(\"🏗️ Creating base models...\")\n",
    "\n",
    "# Model A: Wider architecture, Xavier initialization\n",
    "model_A = MathWordProblemModel(\n",
    "    input_dim=512, \n",
    "    hidden_dim=256, \n",
    "    output_dim=10, \n",
    "    num_layers=3, \n",
    "    model_variant=\"A\"\n",
    ")\n",
    "\n",
    "# Model B: Deeper architecture, He initialization, different activations\n",
    "model_B = MathWordProblemModel(\n",
    "    input_dim=512, \n",
    "    hidden_dim=256, \n",
    "    output_dim=10, \n",
    "    num_layers=4, \n",
    "    model_variant=\"B\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Model A created: {model_A.get_model_info()}\")\n",
    "print(f\"✅ Model B created: {model_B.get_model_info()}\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model_A.eval()\n",
    "model_B.eval()\n",
    "\n",
    "print(\"\\n📊 Model Architecture Summary:\")\n",
    "print(f\"Model A - Total params: {model_A.get_model_info()['total_parameters']:,}\")\n",
    "print(f\"Model B - Total params: {model_B.get_model_info()['total_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04408e60",
   "metadata": {},
   "source": [
    "## Data Generation and Task Definition\n",
    "\n",
    "Create sample inputs and evaluation framework for mathematical reasoning tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39500ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_inputs(task: str = \"math_word_problems\", batch_size: int = 32, input_dim: int = 512) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate sample inputs for mathematical word problems.\n",
    "    In a real scenario, this would load actual preprocessed text embeddings.\n",
    "    \"\"\"\n",
    "    if task == \"math_word_problems\":\n",
    "        # Simulate mathematical problem embeddings with realistic patterns\n",
    "        # Numbers tend to be important in math problems, so we add some structure\n",
    "        \n",
    "        # Base random embeddings\n",
    "        inputs = torch.randn(batch_size, input_dim) * 0.1\n",
    "        \n",
    "        # Add mathematical structure - certain dimensions represent numbers/operations\n",
    "        # Dimensions 0-50: Number representations\n",
    "        inputs[:, :50] = torch.randn(batch_size, 50) * 0.5 + torch.arange(50).float() * 0.01\n",
    "        \n",
    "        # Dimensions 51-100: Operation indicators (+, -, *, /)\n",
    "        operation_patterns = torch.zeros(batch_size, 49)\n",
    "        for i in range(batch_size):\n",
    "            op_type = i % 4  # Cycle through operation types\n",
    "            operation_patterns[i, op_type * 12:(op_type + 1) * 12] = 1.0\n",
    "        inputs[:, 51:100] = operation_patterns\n",
    "        \n",
    "        # Dimensions 101-200: Context embeddings (word meanings)\n",
    "        inputs[:, 101:200] = torch.randn(batch_size, 99) * 0.3\n",
    "        \n",
    "        # Remaining dimensions: General language features\n",
    "        inputs[:, 200:] = torch.randn(batch_size, input_dim - 200) * 0.2\n",
    "        \n",
    "    else:\n",
    "        # Generic task - standard random inputs\n",
    "        inputs = torch.randn(batch_size, input_dim)\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "\n",
    "def generate_ground_truth(inputs: torch.Tensor, task: str = \"math_word_problems\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate ground truth labels for evaluation.\n",
    "    In reality, this would come from your dataset.\n",
    "    \"\"\"\n",
    "    batch_size = inputs.shape[0]\n",
    "    \n",
    "    if task == \"math_word_problems\":\n",
    "        # Generate labels based on input patterns\n",
    "        # Use the mathematical structure we embedded in the inputs\n",
    "        number_features = inputs[:, :50].mean(dim=1)\n",
    "        operation_features = inputs[:, 51:100].argmax(dim=1)\n",
    "        \n",
    "        # Create structured labels (not completely random)\n",
    "        labels = ((number_features * 10 + operation_features) % 10).long()\n",
    "    else:\n",
    "        # Random labels for generic tasks\n",
    "        labels = torch.randint(0, 10, (batch_size,))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate_output(predictions: torch.Tensor, targets: torch.Tensor = None, \n",
    "                   task: str = \"math_word_problems\") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model predictions against ground truth.\n",
    "    Returns comprehensive metrics for comparison.\n",
    "    \"\"\"\n",
    "    if targets is None:\n",
    "        # Generate targets if not provided (for demonstration)\n",
    "        # In real usage, targets would always be provided\n",
    "        batch_size = predictions.shape[0]\n",
    "        targets = torch.randint(0, predictions.shape[1], (batch_size,))\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    if predictions.dim() > 1 and predictions.shape[1] > 1:\n",
    "        pred_classes = torch.argmax(predictions, dim=1)\n",
    "        probabilities = torch.softmax(predictions, dim=1)\n",
    "        confidence = torch.max(probabilities, dim=1)[0]\n",
    "    else:\n",
    "        pred_classes = predictions.round().long().squeeze()\n",
    "        confidence = torch.ones_like(pred_classes).float()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (pred_classes == targets).float().mean().item()\n",
    "    avg_confidence = confidence.mean().item()\n",
    "    \n",
    "    # Additional metrics for comprehensive evaluation\n",
    "    if predictions.dim() > 1:\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=1).mean().item()\n",
    "        top_2_acc = sum((targets.unsqueeze(1) == torch.topk(predictions, 2)[1]).any(dim=1)).float() / len(targets)\n",
    "    else:\n",
    "        entropy = 0.0\n",
    "        top_2_acc = accuracy\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confidence\": avg_confidence,\n",
    "        \"entropy\": entropy,\n",
    "        \"top_2_accuracy\": float(top_2_acc),\n",
    "        \"sample_size\": len(targets)\n",
    "    }\n",
    "\n",
    "\n",
    "# Generate sample data for experiments\n",
    "print(\"📊 Generating sample data for experiments...\")\n",
    "\n",
    "# Sample evaluation on a task  \n",
    "inputs = load_sample_inputs(task=\"math_word_problems\", batch_size=64)\n",
    "targets = generate_ground_truth(inputs, task=\"math_word_problems\")\n",
    "\n",
    "print(f\"✅ Generated inputs: {inputs.shape}\")\n",
    "print(f\"✅ Generated targets: {targets.shape}\")\n",
    "print(f\"📈 Target distribution: {torch.bincount(targets)}\")\n",
    "print(f\"🎯 Input statistics - Mean: {inputs.mean():.4f}, Std: {inputs.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ada016",
   "metadata": {},
   "source": [
    "## Model Evaluation: Individual Performance\n",
    "\n",
    "First, evaluate each base model individually to establish baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Evaluating individual model performance...\")\n",
    "\n",
    "# Get individual model outputs\n",
    "with torch.no_grad():\n",
    "    outputs_A = model_A(inputs)\n",
    "    outputs_B = model_B(inputs)\n",
    "\n",
    "# Evaluate individual models\n",
    "results_A = evaluate_output(outputs_A, targets, task=\"math_word_problems\")\n",
    "results_B = evaluate_output(outputs_B, targets, task=\"math_word_problems\")\n",
    "\n",
    "print(\"\\n📊 Individual Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n🤖 Model A Results:\")\n",
    "for metric, value in results_A.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric:.<20}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:.<20}: {value}\")\n",
    "\n",
    "print(f\"\\n🤖 Model B Results:\")\n",
    "for metric, value in results_B.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric:.<20}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:.<20}: {value}\")\n",
    "\n",
    "# Determine which model performs better\n",
    "better_model = \"A\" if results_A['accuracy'] > results_B['accuracy'] else \"B\"\n",
    "performance_gap = abs(results_A['accuracy'] - results_B['accuracy'])\n",
    "\n",
    "print(f\"\\n🏆 Better performing model: Model {better_model}\")\n",
    "print(f\"📈 Performance gap: {performance_gap:.4f} ({performance_gap*100:.2f}%)\")\n",
    "\n",
    "# Store outputs for fusion experiments\n",
    "outputs_ens = [outputs_A, outputs_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac612ba",
   "metadata": {},
   "source": [
    "## Parameter Merging Implementation\n",
    "\n",
    "Create merged models using different parameter fusion ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_models(model_a: nn.Module, model_b: nn.Module, alpha: float = 0.5) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Merge two models using linear interpolation of parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_a: First model\n",
    "        model_b: Second model  \n",
    "        alpha: Mixing ratio (0.5 = equal mix, 0.7 = 70% model_a + 30% model_b)\n",
    "    \n",
    "    Returns:\n",
    "        Merged model with interpolated parameters\n",
    "    \"\"\"\n",
    "    # Create a new model with the same architecture as model_a\n",
    "    merged_model = MathWordProblemModel(\n",
    "        input_dim=model_a.input_dim,\n",
    "        hidden_dim=model_a.hidden_dim, \n",
    "        output_dim=model_a.output_dim,\n",
    "        model_variant=\"Merged\"\n",
    "    )\n",
    "    \n",
    "    # Merge parameters\n",
    "    merged_state_dict = {}\n",
    "    model_a_state = model_a.state_dict()\n",
    "    model_b_state = model_b.state_dict()\n",
    "    \n",
    "    for key in model_a_state.keys():\n",
    "        if key in model_b_state:\n",
    "            # Linear interpolation of parameters\n",
    "            merged_state_dict[key] = alpha * model_a_state[key] + (1 - alpha) * model_b_state[key]\n",
    "        else:\n",
    "            # If parameter doesn't exist in model_b, use model_a's parameter\n",
    "            merged_state_dict[key] = model_a_state[key]\n",
    "    \n",
    "    # Handle parameters that exist only in model_b\n",
    "    for key in model_b_state.keys():\n",
    "        if key not in merged_state_dict:\n",
    "            merged_state_dict[key] = model_b_state[key]\n",
    "    \n",
    "    merged_model.load_state_dict(merged_state_dict, strict=False)\n",
    "    merged_model.eval()\n",
    "    \n",
    "    return merged_model\n",
    "\n",
    "\n",
    "print(\"⚙️ Creating parameter-merged models...\")\n",
    "\n",
    "# Create merged models with different ratios\n",
    "merged_models = {}\n",
    "merge_ratios = [0.5, 0.3, 0.7]  # 50/50, 30/70, 70/30\n",
    "\n",
    "for alpha in merge_ratios:\n",
    "    print(f\"  🔧 Creating merged model with ratio {alpha:.1f}/{1-alpha:.1f}...\")\n",
    "    merged_model = merge_models(model_A, model_B, alpha=alpha)\n",
    "    \n",
    "    # Update strategy mapping\n",
    "    ratio_str = f\"({int(alpha*100)}/{int((1-alpha)*100)})\"\n",
    "    strategy_name = f\"Parameter Merging {ratio_str}\"\n",
    "    merged_models[strategy_name] = merged_model\n",
    "    \n",
    "    print(f\"    ✅ {strategy_name} model created\")\n",
    "\n",
    "# Update strategies dictionary with merged models\n",
    "for strategy_name, merged_model in merged_models.items():\n",
    "    if strategy_name in all_strategies:\n",
    "        # Create a lambda that captures the merged model\n",
    "        all_strategies[strategy_name] = lambda inputs, model=merged_model: model(inputs)\n",
    "\n",
    "print(f\"\\n✅ Created {len(merged_models)} parameter-merged models\")\n",
    "print(f\"📊 Total fusion strategies ready: {sum(1 for v in all_strategies.values() if v is not None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e8a83",
   "metadata": {},
   "source": [
    "## Comprehensive Fusion Strategy Evaluation\n",
    "\n",
    "Evaluate all fusion strategies following the exact prompt structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop following the exact prompt structure\n",
    "print(\"🧪 Running comprehensive fusion strategy evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "timing_results = {}\n",
    "detailed_metrics = {}\n",
    "\n",
    "for name, func in all_strategies.items():\n",
    "    print(f\"\\n🔬 Evaluating: {name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            if \"Parameter Merging\" in name:\n",
    "                # Parameter merging strategies\n",
    "                if name in merged_models:\n",
    "                    merged_out = merged_models[name](inputs)\n",
    "                    evaluation_result = evaluate_output(merged_out, targets, task=\"math_word_problems\")\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Merged model not found for {name}, skipping...\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Output-level fusion strategies\n",
    "                if func is not None:\n",
    "                    combined = func(outputs_ens)\n",
    "                    evaluation_result = evaluate_output(combined, targets, task=\"math_word_problems\")\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Function not implemented for {name}, skipping...\")\n",
    "                    continue\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = evaluation_result['accuracy']\n",
    "        timing_results[name] = execution_time\n",
    "        detailed_metrics[name] = evaluation_result\n",
    "        \n",
    "        print(f\"  ✅ Accuracy: {evaluation_result['accuracy']:.4f}\")\n",
    "        print(f\"  ⏱️ Time: {execution_time:.6f}s\")\n",
    "        print(f\"  🎯 Confidence: {evaluation_result['confidence']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error evaluating {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🎉 Evaluation completed for {len(results)} strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6b75c",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "Comprehensive analysis of fusion strategy performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66836e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🏆 FUSION STRATEGY COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sort results by accuracy\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n📊 Performance Ranking (Top to Bottom):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for rank, (strategy, accuracy) in enumerate(sorted_results, 1):\n",
    "    timing = timing_results.get(strategy, 0)\n",
    "    metrics = detailed_metrics.get(strategy, {})\n",
    "    \n",
    "    print(f\"{rank:2d}. {strategy:<35} | Acc: {accuracy:.4f} | Time: {timing:.4f}s\")\n",
    "    if metrics:\n",
    "        print(f\"     Confidence: {metrics.get('confidence', 0):.4f} | \"\n",
    "              f\"Top-2 Acc: {metrics.get('top_2_accuracy', 0):.4f} | \"\n",
    "              f\"Entropy: {metrics.get('entropy', 0):.4f}\")\n",
    "\n",
    "# Statistical analysis\n",
    "accuracies = list(results.values())\n",
    "best_accuracy = max(accuracies)\n",
    "worst_accuracy = min(accuracies)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "print(f\"\\n📈 Statistical Summary:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Best Performance:     {best_accuracy:.4f} ({sorted_results[0][0]})\")\n",
    "print(f\"Worst Performance:    {worst_accuracy:.4f} ({sorted_results[-1][0]})\")\n",
    "print(f\"Mean Performance:     {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Deviation:   {std_accuracy:.4f}\")\n",
    "print(f\"Performance Range:    {best_accuracy - worst_accuracy:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(f\"\\n🤖 Comparison with Individual Models:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Model A Accuracy:     {results_A['accuracy']:.4f}\")\n",
    "print(f\"Model B Accuracy:     {results_B['accuracy']:.4f}\")\n",
    "print(f\"Best Fusion:          {best_accuracy:.4f}\")\n",
    "\n",
    "improvement_over_best = best_accuracy - max(results_A['accuracy'], results_B['accuracy'])\n",
    "improvement_over_avg = best_accuracy - (results_A['accuracy'] + results_B['accuracy']) / 2\n",
    "\n",
    "print(f\"Improvement over best individual: {improvement_over_best:.4f} ({improvement_over_best*100:.2f}%)\")\n",
    "print(f\"Improvement over average:        {improvement_over_avg:.4f} ({improvement_over_avg*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Fusion Strategy Comparison Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance comparison bar chart\n",
    "strategies_list = list(results.keys())\n",
    "accuracies_list = list(results.values())\n",
    "timings_list = [timing_results.get(s, 0) for s in strategies_list]\n",
    "\n",
    "# Sort by accuracy for better visualization\n",
    "sorted_indices = np.argsort(accuracies_list)[::-1]\n",
    "sorted_strategies = [strategies_list[i] for i in sorted_indices]\n",
    "sorted_accuracies = [accuracies_list[i] for i in sorted_indices]\n",
    "\n",
    "bars1 = ax1.bar(range(len(sorted_strategies)), sorted_accuracies, \n",
    "                color=['gold' if i == 0 else 'skyblue' for i in range(len(sorted_strategies))])\n",
    "ax1.set_title('Fusion Strategy Performance Comparison', fontweight='bold')\n",
    "ax1.set_xlabel('Fusion Strategy')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xticks(range(len(sorted_strategies)))\n",
    "ax1.set_xticklabels([s.replace(' ', '\\n') for s in sorted_strategies], rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars1, sorted_accuracies)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Performance vs Timing scatter plot\n",
    "colors = ['red' if 'Parameter Merging' in s else 'blue' for s in strategies_list]\n",
    "scatter = ax2.scatter(timings_list, accuracies_list, c=colors, alpha=0.7, s=60)\n",
    "ax2.set_title('Performance vs Execution Time', fontweight='bold')\n",
    "ax2.set_xlabel('Execution Time (seconds)')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend for parameter merging vs output fusion\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', alpha=0.7, label='Parameter Merging'),\n",
    "                  Patch(facecolor='blue', alpha=0.7, label='Output Fusion')]\n",
    "ax2.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "# Annotate best performing points\n",
    "for i, (strategy, acc) in enumerate(zip(strategies_list, accuracies_list)):\n",
    "    if acc > np.percentile(accuracies_list, 80):  # Top 20% performers\n",
    "        ax2.annotate(strategy.split()[0], (timings_list[i], acc), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 3. Confidence vs Accuracy analysis\n",
    "confidences = [detailed_metrics.get(s, {}).get('confidence', 0) for s in strategies_list]\n",
    "ax3.scatter(confidences, accuracies_list, alpha=0.7, s=60)\n",
    "ax3.set_title('Model Confidence vs Accuracy', fontweight='bold')\n",
    "ax3.set_xlabel('Average Prediction Confidence')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "if len(confidences) > 1:\n",
    "    z = np.polyfit(confidences, accuracies_list, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax3.plot(sorted(confidences), p(sorted(confidences)), \"r--\", alpha=0.8)\n",
    "\n",
    "# 4. Strategy type comparison\n",
    "strategy_types = {\n",
    "    'Output Fusion': [],\n",
    "    'Parameter Merging': []\n",
    "}\n",
    "\n",
    "for strategy, acc in results.items():\n",
    "    if 'Parameter Merging' in strategy:\n",
    "        strategy_types['Parameter Merging'].append(acc)\n",
    "    else:\n",
    "        strategy_types['Output Fusion'].append(acc)\n",
    "\n",
    "type_names = list(strategy_types.keys())\n",
    "type_means = [np.mean(strategy_types[t]) if strategy_types[t] else 0 for t in type_names]\n",
    "type_stds = [np.std(strategy_types[t]) if len(strategy_types[t]) > 1 else 0 for t in type_names]\n",
    "\n",
    "bars4 = ax4.bar(type_names, type_means, yerr=type_stds, capsize=5, \n",
    "                color=['lightcoral', 'lightblue'], alpha=0.7)\n",
    "ax4.set_title('Strategy Type Comparison', fontweight='bold')\n",
    "ax4.set_ylabel('Mean Accuracy')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add mean values on bars\n",
    "for bar, mean in zip(bars4, type_means):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e10fa8",
   "metadata": {},
   "source": [
    "## Key Insights and Recommendations\n",
    "\n",
    "Analysis of fusion strategy performance to guide MVP development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎯 KEY INSIGHTS AND RECOMMENDATIONS FOR MVP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze results to provide actionable insights\n",
    "output_fusion_results = {k: v for k, v in results.items() if 'Parameter Merging' not in k}\n",
    "parameter_merging_results = {k: v for k, v in results.items() if 'Parameter Merging' in k}\n",
    "\n",
    "best_output_fusion = max(output_fusion_results.items(), key=lambda x: x[1]) if output_fusion_results else (\"None\", 0)\n",
    "best_parameter_merging = max(parameter_merging_results.items(), key=lambda x: x[1]) if parameter_merging_results else (\"None\", 0)\n",
    "overall_best = max(results.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n🏆 PERFORMANCE WINNERS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Overall Best:           {overall_best[0]} ({overall_best[1]:.4f})\")\n",
    "print(f\"Best Output Fusion:     {best_output_fusion[0]} ({best_output_fusion[1]:.4f})\")\n",
    "print(f\"Best Parameter Merging: {best_parameter_merging[0]} ({best_parameter_merging[1]:.4f})\")\n",
    "\n",
    "# Timing analysis\n",
    "fastest_strategy = min(timing_results.items(), key=lambda x: x[1])\n",
    "slowest_strategy = max(timing_results.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n⚡ TIMING ANALYSIS:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Fastest Strategy:  {fastest_strategy[0]} ({fastest_strategy[1]:.6f}s)\")\n",
    "print(f\"Slowest Strategy:  {slowest_strategy[0]} ({slowest_strategy[1]:.6f}s)\")\n",
    "print(f\"Speed Difference:  {slowest_strategy[1] / fastest_strategy[1]:.1f}x\")\n",
    "\n",
    "# MVP Recommendations\n",
    "print(f\"\\n🚀 MVP DEVELOPMENT RECOMMENDATIONS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Determine best approach\n",
    "if best_parameter_merging[1] > best_output_fusion[1]:\n",
    "    recommendation = \"Parameter Merging\"\n",
    "    reason = f\"Parameter merging achieved {best_parameter_merging[1]:.4f} vs {best_output_fusion[1]:.4f} for output fusion\"\n",
    "else:\n",
    "    recommendation = \"Output Fusion\"\n",
    "    reason = f\"Output fusion achieved {best_output_fusion[1]:.4f} vs {best_parameter_merging[1]:.4f} for parameter merging\"\n",
    "\n",
    "print(f\"1. 🎯 PRIMARY STRATEGY: {recommendation}\")\n",
    "print(f\"   Rationale: {reason}\")\n",
    "\n",
    "# Efficiency consideration\n",
    "output_fusion_times = [timing_results[k] for k in output_fusion_results.keys()]\n",
    "parameter_merging_times = [timing_results[k] for k in parameter_merging_results.keys()]\n",
    "\n",
    "avg_output_time = np.mean(output_fusion_times) if output_fusion_times else 0\n",
    "avg_param_time = np.mean(parameter_merging_times) if parameter_merging_times else 0\n",
    "\n",
    "if avg_output_time < avg_param_time:\n",
    "    efficiency_winner = \"Output Fusion\"\n",
    "    efficiency_factor = avg_param_time / avg_output_time if avg_output_time > 0 else 1\n",
    "else:\n",
    "    efficiency_winner = \"Parameter Merging\" \n",
    "    efficiency_factor = avg_output_time / avg_param_time if avg_param_time > 0 else 1\n",
    "\n",
    "print(f\"\\n2. ⚡ EFFICIENCY CHAMPION: {efficiency_winner}\")\n",
    "print(f\"   Speed advantage: {efficiency_factor:.1f}x faster on average\")\n",
    "\n",
    "# Robustness analysis\n",
    "output_fusion_std = np.std(list(output_fusion_results.values())) if output_fusion_results else 0\n",
    "parameter_merging_std = np.std(list(parameter_merging_results.values())) if parameter_merging_results else 0\n",
    "\n",
    "print(f\"\\n3. 🛡️ ROBUSTNESS ANALYSIS:\")\n",
    "print(f\"   Output Fusion Variance:     {output_fusion_std:.4f}\")\n",
    "print(f\"   Parameter Merging Variance: {parameter_merging_std:.4f}\")\n",
    "\n",
    "if output_fusion_std < parameter_merging_std:\n",
    "    print(f\"   → Output fusion shows more consistent results\")\n",
    "else:\n",
    "    print(f\"   → Parameter merging shows more consistent results\")\n",
    "\n",
    "print(f\"\\n4. 🎛️ OPTIMAL CONFIGURATIONS:\")\n",
    "print(f\"   Best Weighted Average: {max((k,v) for k,v in results.items() if 'Weighted Average' in k)[0]}\")\n",
    "print(f\"   Best Parameter Ratio:  {max((k,v) for k,v in results.items() if 'Parameter Merging' in k)[0]}\")\n",
    "\n",
    "print(f\"\\n5. 💡 IMPLEMENTATION STRATEGY:\")\n",
    "print(f\"   • Start with {overall_best[0]} as primary fusion method\")\n",
    "print(f\"   • Implement {fastest_strategy[0]} for latency-critical scenarios\")\n",
    "print(f\"   • Consider ensemble of top 3 performers for maximum robustness\")\n",
    "print(f\"   • A/B test between parameter merging vs output fusion in production\")\n",
    "\n",
    "# Final verdict\n",
    "improvement_vs_individual = overall_best[1] - max(results_A['accuracy'], results_B['accuracy'])\n",
    "\n",
    "print(f\"\\n✅ EXPERIMENT CONCLUSION:\")\n",
    "print(\"-\" * 30)\n",
    "if improvement_vs_individual > 0:\n",
    "    print(f\"🎉 Model fusion IS beneficial! Best fusion improves performance by {improvement_vs_individual:.4f} ({improvement_vs_individual*100:.2f}%)\")\n",
    "    print(f\"📈 Recommended approach: {overall_best[0]}\")\n",
    "else:\n",
    "    print(f\"⚠️ Model fusion shows minimal benefit. Individual models may be sufficient.\")\n",
    "    print(f\"🤔 Consider: Better base models, different fusion techniques, or task-specific optimization\")\n",
    "\n",
    "print(f\"\\n🔬 Next steps: Scale experiment to larger datasets and production workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2a421",
   "metadata": {},
   "source": [
    "## Export Results for Further Analysis\n",
    "\n",
    "Save experimental results for documentation and future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "experiment_summary = {\n",
    "    \"experiment_info\": {\n",
    "        \"date\": str(pd.Timestamp.now()),\n",
    "        \"task\": \"math_word_problems\", \n",
    "        \"sample_size\": len(targets),\n",
    "        \"input_dimension\": inputs.shape[1],\n",
    "        \"num_strategies\": len(results)\n",
    "    },\n",
    "    \"model_architectures\": {\n",
    "        \"model_a\": model_A.get_model_info(),\n",
    "        \"model_b\": model_B.get_model_info()\n",
    "    },\n",
    "    \"individual_performance\": {\n",
    "        \"model_a\": results_A,\n",
    "        \"model_b\": results_B\n",
    "    },\n",
    "    \"fusion_results\": {\n",
    "        strategy: {\n",
    "            \"accuracy\": results[strategy],\n",
    "            \"execution_time\": timing_results.get(strategy, 0),\n",
    "            \"detailed_metrics\": detailed_metrics.get(strategy, {})\n",
    "        }\n",
    "        for strategy in results.keys()\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"best_overall\": overall_best[0],\n",
    "        \"best_overall_accuracy\": overall_best[1],\n",
    "        \"best_output_fusion\": best_output_fusion[0],\n",
    "        \"best_parameter_merging\": best_parameter_merging[0],\n",
    "        \"improvement_over_individual\": improvement_vs_individual,\n",
    "        \"recommendation\": recommendation\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "results_file = project_root / \"experiments\" / \"fusion_experiment_results.json\"\n",
    "results_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(experiment_summary, f, indent=2, default=str)\n",
    "\n",
    "# Create DataFrame for easy analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'Strategy': list(results.keys()),\n",
    "    'Accuracy': list(results.values()),\n",
    "    'Execution_Time': [timing_results.get(s, 0) for s in results.keys()],\n",
    "    'Confidence': [detailed_metrics.get(s, {}).get('confidence', 0) for s in results.keys()],\n",
    "    'Type': ['Parameter Merging' if 'Parameter Merging' in s else 'Output Fusion' for s in results.keys()]\n",
    "})\n",
    "\n",
    "# Save DataFrame\n",
    "csv_file = project_root / \"experiments\" / \"fusion_experiment_results.csv\"\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"💾 Experiment results saved:\")\n",
    "print(f\"  📄 JSON: {results_file}\")\n",
    "print(f\"  📊 CSV:  {csv_file}\")\n",
    "\n",
    "# Display final summary table\n",
    "print(f\"\\n📋 FINAL RESULTS SUMMARY:\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False).to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "print(f\"\\n🎊 Model Fusion Comparison Experiment Complete!\")\n",
    "print(f\"✅ Successfully benchmarked {len(results)} fusion strategies\")\n",
    "print(f\"🏆 Winner: {overall_best[0]} with {overall_best[1]:.4f} accuracy\")\n",
    "print(f\"📈 Performance improvement: {improvement_vs_individual*100:.2f}% over best individual model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
