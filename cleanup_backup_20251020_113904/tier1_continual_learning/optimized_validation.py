#!/usr/bin/env python3
"""
Optimized Tier 1 Continual Learning Validation
==============================================

Fast, reliable validation for Tier 1 continual learning benchmarks.
Focuses on working datasets with optimized loading and proper timeouts.

Datasets (all from torchvision - guaranteed to work):
- Fashion-MNIST: Drop-in replacement for MNIST with clothing
- SVHN: Real-world digits, domain shift test  
- EMNIST: Extended MNIST with letters
- CIFAR-10: Core small-image benchmark
- CIFAR-100: 100-class version of CIFAR

Features:
- Fast dataset loading with timeouts
- Real continual learning benchmarks
- Forgetting resistance measurement
- Forward transfer evaluation
- Resource scaling analysis
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime
import json
import sys

# Add project to path
sys.path.append(str(Path(__file__).parent.parent.parent))


@dataclass
class ContinualLearningResult:
    """Results from continual learning evaluation."""
    dataset_name: str
    num_tasks: int
    method: str
    
    # Core metrics
    final_accuracy: float
    average_accuracy: float
    forgetting_measure: float
    forward_transfer: float
    
    # Per-task results
    task_accuracies: List[float] = field(default_factory=list)
    task_times: List[float] = field(default_factory=list)
    
    # Resource metrics
    total_parameters: int = 0
    peak_memory_mb: float = 0.0
    total_training_time: float = 0.0
    
    # Metadata
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    success: bool = True
    error_message: str = ""


class SimpleContinualModel(nn.Module):
    """Simple model for continual learning experiments."""
    
    def __init__(self, input_dim: int, hidden_dim: int = 128, num_classes: int = 10):
        super().__init__()
        self.flatten = nn.Flatten()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(), 
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, x):
        x = self.flatten(x)
        return self.network(x)


class OptimizedTier1Validator:
    """Optimized validator for Tier 1 continual learning benchmarks."""
    
    def __init__(self, device: str = 'auto', data_dir: str = './data'):
        """Initialize validator."""
        # Device setup
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                print("âœ… Using CUDA GPU")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = torch.device('mps')
                print("âœ… Using Apple Silicon GPU (MPS)")
            else:
                self.device = torch.device('cpu')
                print("âœ… Using CPU")
        else:
            self.device = torch.device(device)
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # Available datasets (all from torchvision - guaranteed to work)
        self.available_datasets = {
            'fashion_mnist': {
                'loader': torchvision.datasets.FashionMNIST,
                'transform': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.2860,), (0.3530,))
                ]),
                'num_classes': 10,
                'input_shape': (1, 28, 28)
            },
            'svhn': {
                'loader': torchvision.datasets.SVHN,
                'transform': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                ]),
                'num_classes': 10,
                'input_shape': (3, 32, 32),
                'split_arg': 'split'  # SVHN uses 'split' instead of 'train'
            },
            'emnist': {
                'loader': torchvision.datasets.EMNIST,
                'transform': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.1751,), (0.3332,))
                ]),
                'num_classes': 47,  # balanced split
                'input_shape': (1, 28, 28),
                'extra_args': {'split': 'balanced'}
            },
            'cifar10': {
                'loader': torchvision.datasets.CIFAR10,
                'transform': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
                ]),
                'num_classes': 10,
                'input_shape': (3, 32, 32)
            },
            'cifar100': {
                'loader': torchvision.datasets.CIFAR100,
                'transform': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
                ]),
                'num_classes': 100,
                'input_shape': (3, 32, 32)
            }
        }
    
    def load_dataset_fast(self, dataset_name: str, train: bool = True, timeout: int = 60) -> torch.utils.data.Dataset:
        """Load dataset with timeout to prevent hanging."""
        if dataset_name not in self.available_datasets:
            raise ValueError(f"Dataset {dataset_name} not available. Options: {list(self.available_datasets.keys())}")
        
        config = self.available_datasets[dataset_name]
        
        print(f"ðŸ“¥ Loading {dataset_name} ({'train' if train else 'test'})...")
        start_time = time.time()
        
        try:
            # Prepare arguments
            args = {
                'root': str(self.data_dir),
                'transform': config['transform'],
                'download': True
            }
            
            # Handle different argument names
            if 'split_arg' in config:
                args['split'] = 'train' if train else 'test'
            else:
                args['train'] = train
            
            # Add extra arguments
            if 'extra_args' in config:
                args.update(config['extra_args'])
            
            # Load dataset
            dataset = config['loader'](**args)
            
            load_time = time.time() - start_time
            print(f"âœ… Loaded {dataset_name}: {len(dataset)} samples in {load_time:.2f}s")
            
            return dataset
            
        except Exception as e:
            print(f"âŒ Failed to load {dataset_name}: {e}")
            raise
    
    def create_continual_tasks(self, dataset: torch.utils.data.Dataset, num_tasks: int = 3) -> List[torch.utils.data.Dataset]:
        """Create continual learning tasks by splitting classes."""
        print(f"ðŸ”„ Creating {num_tasks} continual learning tasks...")
        
        # Get all targets
        if hasattr(dataset, 'targets'):
            targets = dataset.targets
        else:
            # For SVHN and other datasets
            targets = [dataset[i][1] for i in range(len(dataset))]
        
        # Convert to tensor if needed
        if not isinstance(targets, torch.Tensor):
            targets = torch.tensor(targets)
        
        unique_classes = torch.unique(targets).tolist()
        num_classes = len(unique_classes)
        
        classes_per_task = max(1, num_classes // num_tasks)
        
        tasks = []
        for task_idx in range(num_tasks):
            start_class = task_idx * classes_per_task
            if task_idx == num_tasks - 1:
                # Last task gets remaining classes
                end_class = num_classes
            else:
                end_class = (task_idx + 1) * classes_per_task
            
            task_classes = unique_classes[start_class:end_class]
            
            # Find indices for this task
            task_mask = torch.zeros(len(targets), dtype=torch.bool)
            for cls in task_classes:
                task_mask |= (targets == cls)
            
            task_indices = torch.where(task_mask)[0].tolist()
            
            # Create subset
            task_dataset = torch.utils.data.Subset(dataset, task_indices)
            tasks.append(task_dataset)
            
            print(f"   Task {task_idx + 1}: {len(task_indices)} samples, classes {task_classes}")
        
        return tasks
    
    def train_on_task(self, model: nn.Module, task_dataset: torch.utils.data.Dataset, 
                     epochs: int = 2, lr: float = 0.001, batch_size: int = 128) -> float:
        """Train model on a single task."""
        model.train()
        
        # Create data loader
        loader = torch.utils.data.DataLoader(
            task_dataset, batch_size=batch_size, shuffle=True, num_workers=0
        )
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        start_time = time.time()
        total_loss = 0.0
        num_batches = 0
        
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
        
        training_time = time.time() - start_time
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        
        print(f"     Training time: {training_time:.2f}s, Avg loss: {avg_loss:.4f}")
        
        return training_time
    
    def evaluate_on_task(self, model: nn.Module, task_dataset: torch.utils.data.Dataset, 
                        batch_size: int = 128) -> float:
        """Evaluate model on a single task."""
        model.eval()
        
        loader = torch.utils.data.DataLoader(
            task_dataset, batch_size=batch_size, shuffle=False, num_workers=0
        )
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)
        
        accuracy = correct / total if total > 0 else 0.0
        return accuracy
    
    def run_continual_learning_benchmark(self, dataset_name: str, num_tasks: int = 3, 
                                       epochs_per_task: int = 2) -> ContinualLearningResult:
        """Run complete continual learning benchmark."""
        print(f"\n{'='*80}")
        print(f"ðŸ§  TIER 1 CONTINUAL LEARNING BENCHMARK")
        print(f"{'='*80}")
        print(f"ðŸ“Š Dataset: {dataset_name}")
        print(f"ðŸ”„ Tasks: {num_tasks}")
        print(f"âš¡ Epochs per task: {epochs_per_task}")
        
        try:
            # Load dataset
            start_time = time.time()
            
            train_dataset = self.load_dataset_fast(dataset_name, train=True)
            test_dataset = self.load_dataset_fast(dataset_name, train=False)
            
            # Create tasks
            train_tasks = self.create_continual_tasks(train_dataset, num_tasks)
            test_tasks = self.create_continual_tasks(test_dataset, num_tasks)
            
            # Setup model
            config = self.available_datasets[dataset_name]
            input_dim = np.prod(config['input_shape'])
            num_classes = config['num_classes']
            
            model = SimpleContinualModel(input_dim, hidden_dim=128, num_classes=num_classes)
            model = model.to(self.device)
            
            total_params = sum(p.numel() for p in model.parameters())
            print(f"ðŸ—ï¸  Model parameters: {total_params:,}")
            
            # Track results
            task_accuracies = []
            task_times = []
            all_task_accuracies = []  # For forgetting measurement
            
            print(f"\nðŸš€ Starting continual learning...")
            
            # Train and evaluate on each task
            for task_idx in range(num_tasks):
                print(f"\n{'â”€'*60}")
                print(f"ðŸ“š Task {task_idx + 1}/{num_tasks}")
                print(f"{'â”€'*60}")
                
                # Train on current task
                train_time = self.train_on_task(
                    model, train_tasks[task_idx], epochs=epochs_per_task
                )
                task_times.append(train_time)
                
                # Evaluate on all tasks seen so far
                current_task_accs = []
                for eval_task_idx in range(task_idx + 1):
                    acc = self.evaluate_on_task(model, test_tasks[eval_task_idx])
                    current_task_accs.append(acc)
                    print(f"   Task {eval_task_idx + 1} accuracy: {acc:.4f}")
                
                all_task_accuracies.append(current_task_accs)
                
                # Current task accuracy
                current_acc = current_task_accs[-1]
                task_accuracies.append(current_acc)
            
            # Calculate metrics
            final_accuracy = task_accuracies[-1] if task_accuracies else 0.0
            average_accuracy = np.mean(task_accuracies) if task_accuracies else 0.0
            
            # Forgetting measure (average accuracy drop on previous tasks)
            forgetting_measure = 0.0
            if len(all_task_accuracies) > 1:
                forgetting_scores = []
                for task_idx in range(len(all_task_accuracies) - 1):
                    # Compare accuracy on task_idx after learning task_idx vs after learning all tasks
                    initial_acc = all_task_accuracies[task_idx][task_idx]
                    final_acc = all_task_accuracies[-1][task_idx]
                    forgetting = max(0, initial_acc - final_acc)
                    forgetting_scores.append(forgetting)
                forgetting_measure = np.mean(forgetting_scores)
            
            # Forward transfer (accuracy on new task compared to random)
            random_accuracy = 1.0 / num_classes
            if task_accuracies:
                forward_transfer = np.mean([max(0, acc - random_accuracy) for acc in task_accuracies])
            else:
                forward_transfer = 0.0
            
            # Memory usage
            if torch.cuda.is_available():
                peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)
                torch.cuda.reset_peak_memory_stats()
            else:
                peak_memory = 0.0
            
            total_time = time.time() - start_time
            
            # Create result
            result = ContinualLearningResult(
                dataset_name=dataset_name,
                num_tasks=num_tasks,
                method='simple_mlp',
                final_accuracy=final_accuracy,
                average_accuracy=average_accuracy,
                forgetting_measure=forgetting_measure,
                forward_transfer=forward_transfer,
                task_accuracies=task_accuracies,
                task_times=task_times,
                total_parameters=total_params,
                peak_memory_mb=peak_memory,
                total_training_time=total_time,
                success=True
            )
            
            # Print summary
            print(f"\n{'='*80}")
            print(f"ðŸ“Š CONTINUAL LEARNING RESULTS")
            print(f"{'='*80}")
            print(f"ðŸŽ¯ Dataset: {dataset_name}")
            print(f"ðŸ“ˆ Final accuracy: {final_accuracy:.4f}")
            print(f"ðŸ“ˆ Average accuracy: {average_accuracy:.4f}")
            print(f"ðŸ§  Forgetting measure: {forgetting_measure:.4f}")
            print(f"ðŸš€ Forward transfer: {forward_transfer:.4f}")
            print(f"â±ï¸  Total time: {total_time:.2f}s")
            print(f"ðŸ’¾ Peak memory: {peak_memory:.1f}MB")
            
            return result
            
        except Exception as e:
            print(f"âŒ Benchmark failed: {e}")
            return ContinualLearningResult(
                dataset_name=dataset_name,
                num_tasks=num_tasks,
                method='simple_mlp',
                final_accuracy=0.0,
                average_accuracy=0.0,
                forgetting_measure=1.0,
                forward_transfer=0.0,
                success=False,
                error_message=str(e)
            )
    
    def run_tier1_validation(self, datasets: List[str] = None, quick_mode: bool = True) -> Dict[str, ContinualLearningResult]:
        """Run Tier 1 validation on multiple datasets."""
        if datasets is None:
            if quick_mode:
                datasets = ['fashion_mnist', 'cifar10']  # Fast datasets
            else:
                datasets = list(self.available_datasets.keys())
        
        print(f"\n{'='*80}")
        print(f"ðŸ§  TIER 1 CONTINUAL LEARNING VALIDATION")
        print(f"{'='*80}")
        print(f"ðŸ“Š Datasets: {', '.join(datasets)}")
        print(f"âš¡ Mode: {'Quick' if quick_mode else 'Comprehensive'}")
        
        results = {}
        
        for i, dataset_name in enumerate(datasets, 1):
            print(f"\n{'â”€'*80}")
            print(f"DATASET {i}/{len(datasets)}: {dataset_name.upper()}")
            print(f"{'â”€'*80}")
            
            # Run benchmark
            result = self.run_continual_learning_benchmark(
                dataset_name, 
                num_tasks=3 if quick_mode else 5,
                epochs_per_task=2 if quick_mode else 5
            )
            
            results[dataset_name] = result
            
            if result.success:
                print(f"âœ… {dataset_name}: {result.average_accuracy:.4f} avg accuracy")
            else:
                print(f"âŒ {dataset_name}: {result.error_message}")
        
        # Summary
        print(f"\n{'='*80}")
        print(f"ðŸŽ‰ TIER 1 VALIDATION COMPLETE")
        print(f"{'='*80}")
        
        successful_results = [r for r in results.values() if r.success]
        
        if successful_results:
            avg_accuracy = np.mean([r.average_accuracy for r in successful_results])
            avg_forgetting = np.mean([r.forgetting_measure for r in successful_results])
            avg_forward_transfer = np.mean([r.forward_transfer for r in successful_results])
            
            print(f"ðŸ“Š Summary ({len(successful_results)}/{len(datasets)} successful):")
            print(f"   Average accuracy: {avg_accuracy:.4f}")
            print(f"   Average forgetting: {avg_forgetting:.4f}")
            print(f"   Average forward transfer: {avg_forward_transfer:.4f}")
            
            # Success assessment
            if avg_accuracy >= 0.85 and avg_forgetting <= 0.15:
                print(f"ðŸ† SUCCESS LEVEL: EXCELLENT")
            elif avg_accuracy >= 0.70 and avg_forgetting <= 0.30:
                print(f"âš ï¸  SUCCESS LEVEL: GOOD")
            else:
                print(f"âŒ SUCCESS LEVEL: NEEDS WORK")
        else:
            print(f"âŒ All benchmarks failed")
        
        return results
    
    def save_results(self, results: Dict[str, ContinualLearningResult], output_file: str = None):
        """Save results to JSON file."""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"tier1_continual_learning_results_{timestamp}.json"
        
        # Convert to serializable format
        serializable_results = {}
        for dataset_name, result in results.items():
            serializable_results[dataset_name] = {
                'dataset_name': result.dataset_name,
                'num_tasks': result.num_tasks,
                'method': result.method,
                'final_accuracy': result.final_accuracy,
                'average_accuracy': result.average_accuracy,
                'forgetting_measure': result.forgetting_measure,
                'forward_transfer': result.forward_transfer,
                'task_accuracies': result.task_accuracies,
                'task_times': result.task_times,
                'total_parameters': result.total_parameters,
                'peak_memory_mb': result.peak_memory_mb,
                'total_training_time': result.total_training_time,
                'timestamp': result.timestamp,
                'success': result.success,
                'error_message': result.error_message
            }
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"ðŸ’¾ Results saved to: {output_path}")


def main():
    """Main function for Tier 1 continual learning validation."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Tier 1 Continual Learning Validation')
    parser.add_argument('--dataset', type=str, choices=['fashion_mnist', 'svhn', 'emnist', 'cifar10', 'cifar100'],
                       help='Single dataset to test')
    parser.add_argument('--quick', action='store_true', help='Quick mode (fewer epochs, fewer tasks)')
    parser.add_argument('--device', type=str, default='auto', help='Device to use (auto, cpu, cuda, mps)')
    parser.add_argument('--output', type=str, help='Output file for results')
    
    args = parser.parse_args()
    
    print("ðŸ§  TIER 1 CONTINUAL LEARNING VALIDATION")
    print("="*80)
    
    # Create validator
    validator = OptimizedTier1Validator(device=args.device)
    
    # Run validation
    if args.dataset:
        datasets = [args.dataset]
    else:
        datasets = None  # Use default
    
    results = validator.run_tier1_validation(datasets=datasets, quick_mode=args.quick)
    
    # Save results
    validator.save_results(results, args.output)
    
    print("\nâœ… Tier 1 validation complete!")


if __name__ == '__main__':
    main()